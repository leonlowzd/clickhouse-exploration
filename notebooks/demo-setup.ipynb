{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ba766f",
   "metadata": {},
   "source": [
    "# Mock Location Data to ClickHouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2ddbe",
   "metadata": {},
   "source": [
    "This notebooks gives a good stripped down appreciation on STARLAKE's ETL work. The diagram below shows the high-level diagram of this example:\n",
    "\n",
    "![ image](diagram.jpg)\n",
    "\n",
    "We can start by installing all dependencies required to run this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5b668-948e-4095-88a7-1ec9eadf040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245e778-7480-4bbb-82a7-5219ce437574",
   "metadata": {},
   "source": [
    "## 0. Import all libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dba735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from clickhouse_driver import Client\n",
    "from confluent_kafka import Producer\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cfefc",
   "metadata": {},
   "source": [
    "## 1. Connect to ClickHouse DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158befe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clickhouse_host = \"clickhouse\"\n",
    "clickhouse_port = 9000\n",
    "clickhouse_user = \"default\"\n",
    "clickhouse_pass = \"plschangeme\"\n",
    "clickhouse_db   = \"testdb\"\n",
    "\n",
    "# Connect to ClickHouse\n",
    "client_ch = Client(\n",
    "    host=clickhouse_host,\n",
    "    port=clickhouse_port,\n",
    "    user=clickhouse_user,\n",
    "    password=clickhouse_pass,\n",
    "    database=clickhouse_db\n",
    ")\n",
    "print(\"Connected to ClickHouse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de92fd6e",
   "metadata": {},
   "source": [
    "## 2. Create Table in Clickhouse\n",
    "\n",
    "We can assume all schemas across the sources have already been unified into:\n",
    "\n",
    "\n",
    "| Column Name | Data Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **id** | `UInt32` | Unique identifier. Can take that users can use this unique ID to join with other source specific cols from another table |\n",
    "| **vesselname** | `String` | Name of the vessel. |\n",
    "| **latitude** | `Float64` | Vessel's latitude, typically within the range $[-90, 90]$. |\n",
    "| **longitude** | `Float64` | Vessel's longitude, typically within the range $[-180, 180]$. |\n",
    "| **speed** | `Float64` | Vessel's speed, often in **knots** or another unit. |\n",
    "| **course** | `Float64` | Vessel's **Course Over Ground (COG)**, measured in degrees ($0^{\\circ}$ to $360^{\\circ}$). |\n",
    "| **captureddatetime** | `UInt64` | The time the data was captured, represented as an **epoch in milliseconds** ($\\text{epochms}$) in the UTC+0 timezone. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2573a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ClickHouse: Drop + Create table\n",
    "client_ch.execute(\"DROP TABLE IF EXISTS raw_ais_data_mock\")\n",
    "\n",
    "create_table_ch = \"\"\"\n",
    "CREATE TABLE raw_data_mock (\n",
    "    id                UInt32,\n",
    "    vesselname        String,\n",
    "    latitude          Float64,        -- Latitude [-90, 90]\n",
    "    longitude         Float64,        -- Longitude [-180, 180]\n",
    "    speed             Float64,        -- Speed in knots or other unit\n",
    "    course            Float64,        -- Course over ground (degrees)\n",
    "    captureddatetime  UInt64          -- epochms UTC+0 timezome\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY (vesselname, captureddatetime)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "client_ch.execute(create_table_ch)\n",
    "print(\"Created ClickHouse ais_data table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d15de",
   "metadata": {},
   "source": [
    "## 3. Generate Dummy Data Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ecdf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Updated Avro Schema\n",
    "ais_avro_schema = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"AISData\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"id\", \"type\": \"int\"},\n",
    "    {\"name\": \"vesselname\", \"type\": \"string\"},\n",
    "    {\"name\": \"latitude\", \"type\": \"double\"},\n",
    "    {\"name\": \"longitude\", \"type\": \"double\"},\n",
    "    {\"name\": \"speed\", \"type\": \"double\"},\n",
    "    {\"name\": \"course\", \"type\": \"double\"},\n",
    "    {\"name\": \"captureddatetime\", \"type\": \"long\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "vessel_names = [\n",
    "    \"The Sovereign Tide\",\n",
    "    \"Trident's Reach\",\n",
    "    \"The Albatross Crest\",\n",
    "    \"The Valiant Star\",\n",
    "    \"Kingston Trader\",\n",
    "    \"The Meridian Queen\",\n",
    "    \"The Iron Duke\",\n",
    "    \"MV Apex Voyager\",\n",
    "    \"Oceanic Transporter 9\",\n",
    "    \"CS Zenith Horizon\",\n",
    "    \"Pacific Hauler\",\n",
    "    \"Orion Global\",\n",
    "    \"Stratton Logistics\",\n",
    "    \"The Titanus\",\n",
    "    \"The Nomad's Quest\",\n",
    "    \"Arctic Tern\"\n",
    "]\n",
    "\n",
    "# Updated generate_ais_message function\n",
    "def generate_ais_message(record_id):\n",
    "    \"\"\"Generate a single random AIS message as a dictionary with epoch timestamp.\"\"\"\n",
    "    vesselname = random.choice(vessel_names)\n",
    "    latitude = random.uniform(-90, 90)\n",
    "    longitude = random.uniform(-180, 180)\n",
    "    speed = random.uniform(0, 30)\n",
    "    course = random.uniform(0, 359.99)\n",
    "    now = datetime.now()\n",
    "    random_time = now - timedelta(days=random.random()*7, seconds=random.randint(0, 86400))\n",
    "    \n",
    "    return {\n",
    "        \"id\": record_id,\n",
    "        \"vesselname\": vesselname,\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"speed\": speed,\n",
    "        \"course\": course,\n",
    "        \"captureddatetime\": int(random_time.timestamp() * 1000) # Epoch time in milliseconds\n",
    "    }\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# --- 2. Kafka and Schema Registry Configuration ---\n",
    "kafka_bootstrap_servers = \"broker:29092\"\n",
    "schema_registry_url = \"http://schema-registry:8081\"\n",
    "kafka_topic = \"raw_data_mock\"\n",
    "\n",
    "# Configure Schema Registry Client\n",
    "schema_registry_conf = {'url': schema_registry_url}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "# Configure Avro Serializer\n",
    "# The serializer will use the schema registry to encode messages\n",
    "avro_serializer = AvroSerializer(\n",
    "    schema_registry_client=schema_registry_client,\n",
    "    schema_str=ais_avro_schema\n",
    ")\n",
    "\n",
    "# Configure Kafka Producer\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': kafka_bootstrap_servers,\n",
    "    'acks': 'all',\n",
    "    'compression.type': 'snappy',\n",
    "}\n",
    "producer = Producer(producer_conf)\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    \"\"\"Callback function to report message delivery status.\"\"\"\n",
    "    if err is not None:\n",
    "        print(f'Message delivery failed: {err}')\n",
    "    else:\n",
    "        pass\n",
    "        # print(f'Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347036b",
   "metadata": {},
   "source": [
    "## 4. Insert into Kafka\n",
    "Adjust the loop according to write data into Kafka Broker, can adjust the number of messages according.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fae02-919c-4b10-b8c6-e48534e5110e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 4. Main Production Loop ---\n",
    "total_messages = 1000000\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(total_messages):\n",
    "    message = generate_ais_message(i + 1)\n",
    "    \n",
    "    # Use the Avro serializer to encode the message\n",
    "    producer.produce(\n",
    "        topic=kafka_topic,\n",
    "        value=avro_serializer(message, SerializationContext(kafka_topic, MessageField.VALUE)),\n",
    "        callback=delivery_report\n",
    "    )\n",
    "    \n",
    "    # Poll to trigger delivery callbacks\n",
    "    producer.poll(0)\n",
    "\n",
    "# Wait for any outstanding messages to be delivered\n",
    "producer.flush()\n",
    "end_time = time.time()\n",
    "print(f\"Produced {total_messages} Avro messages to Kafka in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96621f21-af7f-4f18-86d1-2bd114e5830d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Create Sink Connector to write to ClickHouse Table\n",
    "\n",
    "Sink Data into ClickHouse via kafka HTTP Sink Connector. Ensure that Kafka Connect have the HTTP drivers. Can read more on [Clickhouse Website](https://clickhouse.com/docs/integrations/kafka/cloud/confluent/http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450c4817-53a9-484a-9f96-ec63a5a245a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "connect_url_base = \"http://connect:8083/connectors\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026fe49-44a2-4647-aad7-6cd7166eb398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Update Connector for ClickHouse ---\n",
    "connector_name_ch = \"ais-data-ch-sink-connector-new\"\n",
    "url_ch = f\"{connect_url_base}/{connector_name_ch}/config\"\n",
    "\n",
    "# Your provided config for ClickHouse\n",
    "data_ch = {\n",
    "    \"connector.class\": \"com.clickhouse.kafka.connect.ClickHouseSinkConnector\",\n",
    "    \"tasks.max\": \"1\",\n",
    "    \"topics\": \"raw_data_mock\",\n",
    "    # The dedicated connector uses its own connection properties\n",
    "    \"hostname\": \"clickhouse\",\n",
    "    \"port\": \"8123\",\n",
    "    \"username\": \"default\",\n",
    "    \"password\": \"plschangeme\",\n",
    "    \"database\": \"testdb\",\n",
    "    # Avro converter is the right choice for this connector\n",
    "    \"value.converter\": \"io.confluent.connect.avro.AvroConverter\",\n",
    "    \"value.converter.schema.registry.url\": \"http://schema-registry:8081\",\n",
    "    # Avro records contain the schema so these are not needed\n",
    "    \"auto.create\": \"false\",\n",
    "    \"auto.evolve\": \"false\"\n",
    "}\n",
    "\n",
    "response_ch = requests.put(url_ch, headers=headers, json=data_ch)\n",
    "if response_ch.status_code == 201 or response_ch.status_code == 200:\n",
    "    print(\"ClickHouse Sink Connector updated successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to update ClickHouse connector: {response_ch.status_code}\")\n",
    "    print(response_ch.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8494d6-2505-40fb-aa0a-bd31af8d1ff1",
   "metadata": {},
   "source": [
    "## 6. Sample Benchmark Query to count number of records within a bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad885b6e-5fb1-42f6-b5e6-c071931b3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "\n",
    "# Query 1: Count all records within a bounding box\n",
    "query1 = '''\n",
    "SELECT count(*) FROM raw_data_mock WHERE latitude BETWEEN 0 AND 10 AND longitude BETWEEN 0 AND 10\n",
    "    AND captureddatetime > 1503475512418\n",
    "    AND captureddatetime < 1855475512418\n",
    "'''\n",
    "time_pg_q1 = 0\n",
    "time_ch_q1 = 0\n",
    "\n",
    "# Variables to store the counts for comparison\n",
    "count_ch = 0\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start = time.time()\n",
    "    result_ch = client_ch.execute(query1)\n",
    "    # The result is a list of tuples, so we extract the count\n",
    "    count_ch = result_ch[0][0]\n",
    "    time_ch_q1 += time.time() - start\n",
    "\n",
    "print(f\"Average time for ClickHouse: {time_ch_q1/num_runs:.4f} seconds\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Count for ClickHouse: {count_ch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1716f51-d9a2-4ef5-be8d-3fb346fac6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
